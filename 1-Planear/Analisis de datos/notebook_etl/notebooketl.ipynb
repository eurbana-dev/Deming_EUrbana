{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d66d881",
   "metadata": {},
   "source": [
    "# ETL (Extract, Transform, Load) para An√°lisis de Luminarias Urbanas\n",
    "\n",
    "## üìã Introducci√≥n\n",
    "\n",
    "Este notebook implementa un proceso **ETL completo** para el an√°lisis de luminarias urbanas del proyecto ECOLUZ. El proceso integra tanto an√°lisis **supervisado** como **no supervisado** para el mantenimiento predictivo de luminarias.\n",
    "\n",
    "### üéØ Objetivos del ETL\n",
    "\n",
    "1. **Extraer** datos de MongoDB de luminarias urbanas\n",
    "2. **Transformar** los datos aplicando limpieza, normalizaci√≥n y ingenier√≠a de caracter√≠sticas\n",
    "3. **Cargar** los datos procesados para an√°lisis supervisado y no supervisado\n",
    "4. **Generar insights** mediante visualizaciones y modelos predictivos\n",
    "\n",
    "### üîÑ Flujo del Proceso\n",
    "\n",
    "```\n",
    "MongoDB ‚Üí Extract ‚Üí Clean ‚Üí Feature Engineering ‚Üí Supervised Analysis ‚Üí Unsupervised Analysis ‚Üí Insights\n",
    "```\n",
    "\n",
    "### üìä Metodolog√≠as Implementadas\n",
    "\n",
    "- **An√°lisis Supervisado**: Random Forest para clasificaci√≥n del estado de luminarias\n",
    "- **An√°lisis No Supervisado**: K-Means clustering y PCA para identificaci√≥n de patrones\n",
    "- **Visualizaci√≥n Avanzada**: Gr√°ficos de importancia, matrices de confusi√≥n y clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bae2e0",
   "metadata": {},
   "source": [
    "## üìö Marco Te√≥rico\n",
    "\n",
    "### ETL en el Contexto de Machine Learning\n",
    "\n",
    "**ETL (Extract, Transform, Load)** es fundamental para el an√°lisis de datos de calidad:\n",
    "\n",
    "- **Extract**: Extracci√≥n de datos desde fuentes heterog√©neas (MongoDB, APIs, archivos)\n",
    "- **Transform**: Limpieza, normalizaci√≥n, agregaci√≥n y ingenier√≠a de caracter√≠sticas\n",
    "- **Load**: Carga de datos limpios en estructuras optimizadas para an√°lisis\n",
    "\n",
    "### An√°lisis Supervisado vs No Supervisado\n",
    "\n",
    "#### üéØ **An√°lisis Supervisado**\n",
    "- **Objetivo**: Predecir el estado de luminarias (bueno, regular, malo)\n",
    "- **Algoritmo**: Random Forest Classifier\n",
    "- **Ventajas**: \n",
    "  - Maneja variables mixtas (num√©ricas y categ√≥ricas)\n",
    "  - Proporciona importancia de variables\n",
    "  - Robusto ante outliers\n",
    "- **M√©tricas**: Accuracy, Precision, Recall, F1-Score\n",
    "\n",
    "#### üîç **An√°lisis No Supervisado**\n",
    "- **Objetivo**: Descubrir patrones ocultos y agrupamientos naturales\n",
    "- **Algoritmos**: K-Means clustering, PCA (Principal Component Analysis)\n",
    "- **Ventajas**:\n",
    "  - No requiere etiquetas predefinidas\n",
    "  - Identifica estructuras latentes en los datos\n",
    "  - √ötil para segmentaci√≥n y reducci√≥n dimensional\n",
    "- **M√©tricas**: Silhouette Score, Within-Cluster Sum of Squares (WCSS)\n",
    "\n",
    "### T√©cnicas de Visualizaci√≥n\n",
    "\n",
    "1. **Importancia de Variables**: Identifica caracter√≠sticas m√°s relevantes\n",
    "2. **Matriz de Confusi√≥n**: Eval√∫a rendimiento de clasificaci√≥n\n",
    "3. **PCA 2D**: Visualiza clusters en espacio reducido\n",
    "4. **M√©todo del Codo**: Determina n√∫mero √≥ptimo de clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e51d6d0",
   "metadata": {},
   "source": [
    "## üì¶ Importaci√≥n de Librer√≠as\n",
    "\n",
    "Importamos todas las librer√≠as necesarias para el proceso ETL completo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cf79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# LIBRER√çAS PARA PROCESAMIENTO DE DATOS\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from math import pi\n",
    "\n",
    "# ==========================================\n",
    "# CONEXI√ìN A BASE DE DATOS\n",
    "# ==========================================\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# ==========================================\n",
    "# VISUALIZACI√ìN\n",
    "# ==========================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# ==========================================\n",
    "# MACHINE LEARNING - SUPERVISADO\n",
    "# ==========================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# MACHINE LEARNING - NO SUPERVISADO\n",
    "# ==========================================\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# ==========================================\n",
    "# AN√ÅLISIS ESTAD√çSTICO\n",
    "# ==========================================\n",
    "from scipy import stats\n",
    "\n",
    "print(\"‚úÖ Todas las librer√≠as importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407ea397",
   "metadata": {},
   "source": [
    "## üîå FASE 1: EXTRACT (Extracci√≥n)\n",
    "\n",
    "### Configuraci√≥n de Conexi√≥n a MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db1a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar variables de entorno\n",
    "load_dotenv()\n",
    "MONGO_URI = os.getenv(\"CONECTION_DB\")\n",
    "\n",
    "# Validar conexi√≥n\n",
    "if MONGO_URI:\n",
    "    print(\"‚úÖ URI de MongoDB cargada correctamente\")\n",
    "    print(f\"üîó Conectando a: {MONGO_URI[:20]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No se encontr√≥ la URI de MongoDB\")\n",
    "    print(\"üí° Aseg√∫rate de tener un archivo .env con CONECTION_DB\")\n",
    "\n",
    "# Test de conexi√≥n\n",
    "try:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    client.admin.command('ping')\n",
    "    print(\"‚úÖ Conexi√≥n a MongoDB exitosa\")\n",
    "    client.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error de conexi√≥n: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70edbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_mongo_sample_to_csv(\n",
    "    db_name: str,\n",
    "    collection_name: str,\n",
    "    out_csv: str = \"dataset_sample.csv\",\n",
    "    sample_size: int = 100000,\n",
    "    projection: dict | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Funci√≥n ETL para extraer datos de MongoDB y convertirlos a CSV\n",
    "    \n",
    "    Args:\n",
    "        db_name: Nombre de la base de datos\n",
    "        collection_name: Nombre de la colecci√≥n\n",
    "        out_csv: Archivo CSV de salida\n",
    "        sample_size: N√∫mero de documentos a extraer\n",
    "        projection: Campos espec√≠ficos a proyectar\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: Datos extra√≠dos\n",
    "    \"\"\"\n",
    "    \n",
    "    # ‚úÖ Optimizaci√≥n: Usar cach√© local si ya existe\n",
    "    if os.path.exists(out_csv):\n",
    "        print(f\"üìÇ Archivo {out_csv} ya existe, cargando desde cach√©...\")\n",
    "        df = pd.read_csv(out_csv)\n",
    "        print(f\"‚úÖ Cargados {len(df):,} registros desde CSV existente\")\n",
    "        print(f\"üìä Columnas disponibles: {len(df.columns)}\")\n",
    "        return df\n",
    "\n",
    "    # üîå Conectar a MongoDB\n",
    "    print(f\"üîÑ Conectando a MongoDB: {db_name}.{collection_name}\")\n",
    "    mongo_uri = os.getenv(\"CONECTION_DB\")\n",
    "    client = MongoClient(mongo_uri)\n",
    "    col = client[db_name][collection_name]\n",
    "\n",
    "    # üìù Pipeline de agregaci√≥n\n",
    "    pipeline = [{\"$sample\": {\"size\": sample_size}}]\n",
    "    if projection:\n",
    "        pipeline.append({\"$project\": projection})\n",
    "        print(f\"üéØ Proyecci√≥n aplicada: {list(projection.keys())}\")\n",
    "\n",
    "    # üì• Extraer datos\n",
    "    print(f\"üìä Extrayendo muestra de {sample_size:,} documentos...\")\n",
    "    cursor = col.aggregate(pipeline)\n",
    "    data = list(cursor)\n",
    "    \n",
    "    # üîÑ Transformar a DataFrame\n",
    "    df = pd.json_normalize(data)\n",
    "    \n",
    "    # üíæ Guardar cach√©\n",
    "    df.to_csv(out_csv, index=False)\n",
    "    client.close()\n",
    "    \n",
    "    print(f\"‚úÖ Nueva muestra guardada en {out_csv}\")\n",
    "    print(f\"üìä Dimensiones: {df.shape[0]:,} filas √ó {df.shape[1]} columnas\")\n",
    "    print(f\"üíæ Tama√±o del archivo: {os.path.getsize(out_csv)/1024/1024:.2f} MB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ Funci√≥n de extracci√≥n definida correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4ab60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ EJECUTAR EXTRACCI√ìN DE DATOS\n",
    "print(\"üîÑ Iniciando proceso de extracci√≥n...\")\n",
    "\n",
    "df_raw = export_mongo_sample_to_csv(\n",
    "    db_name=\"luminarias\",\n",
    "    collection_name=\"luminarias\",\n",
    "    out_csv=\"luminarias_dataset.csv\",\n",
    "    sample_size=5000\n",
    ")\n",
    "\n",
    "# üëÄ Vista previa de los datos extra√≠dos\n",
    "print(\"\\nüìã RESUMEN DE DATOS EXTRA√çDOS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"‚Ä¢ Filas: {df_raw.shape[0]:,}\")\n",
    "print(f\"‚Ä¢ Columnas: {df_raw.shape[1]}\")\n",
    "print(f\"‚Ä¢ Memoria: {df_raw.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
    "\n",
    "print(\"\\nüîç PRIMERAS 5 FILAS:\")\n",
    "display(df_raw.head())\n",
    "\n",
    "print(\"\\nüìä INFORMACI√ìN DEL DATASET:\")\n",
    "print(df_raw.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ea55f",
   "metadata": {},
   "source": [
    "## üîÑ FASE 2: TRANSFORM (Transformaci√≥n)\n",
    "\n",
    "Esta fase incluye limpieza, normalizaci√≥n, ingenier√≠a de caracter√≠sticas y preparaci√≥n de datos para ambos an√°lisis (supervisado y no supervisado)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e579bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßπ LIMPIEZA Y PREPARACI√ìN DE DATOS\n",
    "print(\"üîÑ Iniciando transformaci√≥n de datos...\")\n",
    "\n",
    "# Copiar dataset para transformaci√≥n\n",
    "df = df_raw.copy()\n",
    "\n",
    "print(f\"üìä Dataset original: {df.shape}\")\n",
    "\n",
    "# ==========================================\n",
    "# 1. AN√ÅLISIS DE CALIDAD DE DATOS\n",
    "# ==========================================\n",
    "print(\"\\nüîç AN√ÅLISIS DE CALIDAD DE DATOS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Valores nulos\n",
    "null_counts = df.isnull().sum()\n",
    "null_percentage = (null_counts / len(df)) * 100\n",
    "quality_report = pd.DataFrame({\n",
    "    'Columna': null_counts.index,\n",
    "    'Valores_Nulos': null_counts.values,\n",
    "    'Porcentaje_Nulos': null_percentage.values\n",
    "}).sort_values('Porcentaje_Nulos', ascending=False)\n",
    "\n",
    "print(\"üìã Top 10 columnas con m√°s valores nulos:\")\n",
    "display(quality_report.head(10))\n",
    "\n",
    "# ==========================================\n",
    "# 2. SELECCI√ìN DE CARACTER√çSTICAS\n",
    "# ==========================================\n",
    "print(\"\\nüéØ SELECCI√ìN DE CARACTER√çSTICAS RELEVANTES:\")\n",
    "\n",
    "# Caracter√≠sticas para an√°lisis supervisado y no supervisado\n",
    "features_supervisado = [\n",
    "    \"potencia_watts\",\n",
    "    \"altura_metros\", \n",
    "    \"consumo.actual_watts\",\n",
    "    \"consumo.acumulado_kwh.mes\",\n",
    "    \"consumo.acumulado_kwh.semana\",\n",
    "    \"consumo.acumulado_kwh.dia\",\n",
    "    \"sensores.luminosidad_lux\",\n",
    "    \"sensores.temperatura_c\",\n",
    "    \"sensores.humedad_pct\",\n",
    "    \"sensores.movimiento\",\n",
    "    \"conectividad.latencia_ms\",\n",
    "    \"eficiencia.lumens_por_watt\",\n",
    "    \"eficiencia.horas_funcionamiento_total\",\n",
    "    \"eficiencia.vida_util_restante_pct\",\n",
    "    \"estado_lampara\"  # Variable objetivo\n",
    "]\n",
    "\n",
    "features_no_supervisado = [\n",
    "    \"potencia_watts\",\n",
    "    \"altura_metros\",\n",
    "    \"consumo.actual_watts\", \n",
    "    \"consumo.acumulado_kwh.mes\",\n",
    "    \"sensores.luminosidad_lux\",\n",
    "    \"sensores.temperatura_c\",\n",
    "    \"sensores.humedad_pct\",\n",
    "    \"conectividad.latencia_ms\",\n",
    "    \"eficiencia.lumens_por_watt\",\n",
    "    \"eficiencia.horas_funcionamiento_total\",\n",
    "    \"eficiencia.vida_util_restante_pct\"\n",
    "]\n",
    "\n",
    "# Verificar disponibilidad de caracter√≠sticas\n",
    "available_features_sup = [col for col in features_supervisado if col in df.columns]\n",
    "available_features_unsup = [col for col in features_no_supervisado if col in df.columns]\n",
    "\n",
    "print(f\"‚úÖ Caracter√≠sticas disponibles para an√°lisis supervisado: {len(available_features_sup)}/{len(features_supervisado)}\")\n",
    "print(f\"‚úÖ Caracter√≠sticas disponibles para an√°lisis no supervisado: {len(available_features_unsup)}/{len(features_no_supervisado)}\")\n",
    "\n",
    "if len(available_features_sup) < len(features_supervisado):\n",
    "    missing_sup = set(features_supervisado) - set(available_features_sup)\n",
    "    print(f\"‚ö†Ô∏è Caracter√≠sticas faltantes (supervisado): {missing_sup}\")\n",
    "\n",
    "if len(available_features_unsup) < len(features_no_supervisado):\n",
    "    missing_unsup = set(features_no_supervisado) - set(available_features_unsup)\n",
    "    print(f\"‚ö†Ô∏è Caracter√≠sticas faltantes (no supervisado): {missing_unsup}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba326b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. LIMPIEZA DE DATOS\n",
    "# ==========================================\n",
    "print(\"\\nüßπ PROCESO DE LIMPIEZA:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Filtrar solo las caracter√≠sticas disponibles\n",
    "df_supervisado = df[available_features_sup].copy() if available_features_sup else pd.DataFrame()\n",
    "df_no_supervisado = df[available_features_unsup].copy() if available_features_unsup else pd.DataFrame()\n",
    "\n",
    "if not df_supervisado.empty:\n",
    "    print(f\"üìä Dataset supervisado: {df_supervisado.shape}\")\n",
    "    \n",
    "    # Eliminar filas con valores nulos en variables cr√≠ticas\n",
    "    antes_limpieza = len(df_supervisado)\n",
    "    df_supervisado = df_supervisado.dropna()\n",
    "    despues_limpieza = len(df_supervisado)\n",
    "    \n",
    "    print(f\"üóëÔ∏è Filas eliminadas por valores nulos: {antes_limpieza - despues_limpieza:,}\")\n",
    "    print(f\"‚úÖ Filas restantes: {despues_limpieza:,}\")\n",
    "\n",
    "if not df_no_supervisado.empty:\n",
    "    print(f\"üìä Dataset no supervisado: {df_no_supervisado.shape}\")\n",
    "    \n",
    "    # Eliminar filas con valores nulos\n",
    "    antes_limpieza_unsup = len(df_no_supervisado)\n",
    "    df_no_supervisado = df_no_supervisado.dropna()\n",
    "    despues_limpieza_unsup = len(df_no_supervisado)\n",
    "    \n",
    "    print(f\"üóëÔ∏è Filas eliminadas por valores nulos: {antes_limpieza_unsup - despues_limpieza_unsup:,}\")\n",
    "    print(f\"‚úÖ Filas restantes: {despues_limpieza_unsup:,}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. INGENIER√çA DE CARACTER√çSTICAS\n",
    "# ==========================================\n",
    "print(\"\\nüîß INGENIER√çA DE CARACTER√çSTICAS:\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "if not df_supervisado.empty and 'estado_lampara' in df_supervisado.columns:\n",
    "    # An√°lisis de la variable objetivo\n",
    "    print(\"üéØ Variable objetivo (estado_lampara):\")\n",
    "    estado_counts = df_supervisado['estado_lampara'].value_counts()\n",
    "    print(estado_counts)\n",
    "    \n",
    "    # Visualizar distribuci√≥n de clases\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    estado_counts.plot(kind='bar', color=['green', 'orange', 'red'])\n",
    "    plt.title('Distribuci√≥n de Estados de Luminarias')\n",
    "    plt.xlabel('Estado')\n",
    "    plt.ylabel('Cantidad')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.pie(estado_counts.values, labels=estado_counts.index, autopct='%1.1f%%', \n",
    "            colors=['green', 'orange', 'red'])\n",
    "    plt.title('Proporci√≥n de Estados')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Estad√≠sticas descriptivas\n",
    "if not df_no_supervisado.empty:\n",
    "    print(\"\\nüìà ESTAD√çSTICAS DESCRIPTIVAS:\")\n",
    "    display(df_no_supervisado.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5060da",
   "metadata": {},
   "source": [
    "## üì• FASE 3: LOAD (Carga y An√°lisis)\n",
    "\n",
    "### üéØ AN√ÅLISIS SUPERVISADO - Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ AN√ÅLISIS SUPERVISADO\n",
    "print(\"üöÄ Iniciando an√°lisis supervisado...\")\n",
    "\n",
    "if not df_supervisado.empty and 'estado_lampara' in df_supervisado.columns:\n",
    "    \n",
    "    # ==========================================\n",
    "    # PREPARACI√ìN DE DATOS\n",
    "    # ==========================================\n",
    "    \n",
    "    # Separar caracter√≠sticas y variable objetivo\n",
    "    X = df_supervisado.drop('estado_lampara', axis=1)\n",
    "    y = df_supervisado['estado_lampara']\n",
    "    \n",
    "    # Codificar variable objetivo si es necesario\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "    \n",
    "    print(f\"‚úÖ Caracter√≠sticas (X): {X.shape}\")\n",
    "    print(f\"‚úÖ Variable objetivo (y): {y.shape}\")\n",
    "    print(f\"üè∑Ô∏è Clases: {le.classes_}\")\n",
    "    \n",
    "    # Escalar caracter√≠sticas\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Divisi√≥n train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä Entrenamiento: {X_train.shape[0]} muestras\")\n",
    "    print(f\"üìä Prueba: {X_test.shape[0]} muestras\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # ENTRENAMIENTO DEL MODELO\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\nüå≥ Entrenando Random Forest...\")\n",
    "    \n",
    "    # Modelo Random Forest\n",
    "    rf_model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    \n",
    "    # ==========================================\n",
    "    # EVALUACI√ìN DEL MODELO\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\nüìä M√âTRICAS DE EVALUACI√ìN:\")\n",
    "    print(\"=\"*30)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"üéØ Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"üéØ Precision: {precision:.3f}\")\n",
    "    print(f\"üéØ Recall: {recall:.3f}\")\n",
    "    print(f\"üéØ F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    print(\"\\nüìã Reporte de clasificaci√≥n:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se puede realizar an√°lisis supervisado: datos insuficientes o variable objetivo faltante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b53afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# VISUALIZACIONES DEL AN√ÅLISIS SUPERVISADO\n",
    "# ==========================================\n",
    "\n",
    "if not df_supervisado.empty and 'estado_lampara' in df_supervisado.columns:\n",
    "    \n",
    "    print(\"\\nüìä GENERANDO VISUALIZACIONES...\")\n",
    "    \n",
    "    # 1. IMPORTANCIA DE VARIABLES\n",
    "    print(\"üìà Gr√°fico 1: Importancia de Variables\")\n",
    "    \n",
    "    # Obtener importancia de caracter√≠sticas\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf_model.feature_importances_\n",
    "    }).sort_values('importance', ascending=True)\n",
    "    \n",
    "    # Gr√°fico de importancia\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(feature_importance['feature'], feature_importance['importance'])\n",
    "    plt.title('Importancia de Variables (Random Forest)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Score de Importancia', fontsize=12)\n",
    "    plt.ylabel('Variables', fontsize=12)\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for i, v in enumerate(feature_importance['importance']):\n",
    "        plt.text(v + 0.001, i, f'{v:.3f}', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('importancia_variables_etl.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. MATRIZ DE CONFUSI√ìN\n",
    "    print(\"üìà Gr√°fico 2: Matriz de Confusi√≥n\")\n",
    "    \n",
    "    # Calcular matriz de confusi√≥n\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Gr√°fico de matriz de confusi√≥n\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=le.classes_, yticklabels=le.classes_,\n",
    "                cbar_kws={'label': 'Cantidad de Predicciones'})\n",
    "    plt.title('Matriz de Confusi√≥n', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Predicci√≥n', fontsize=12)\n",
    "    plt.ylabel('Real', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('matriz_confusion_etl.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar estad√≠sticas de la matriz\n",
    "    print(\"\\\\nüìä An√°lisis de la Matriz de Confusi√≥n:\")\n",
    "    for i, clase in enumerate(le.classes_):\n",
    "        verdaderos_positivos = cm[i, i]\n",
    "        total_reales = cm[i, :].sum()\n",
    "        total_predichos = cm[:, i].sum()\n",
    "        \n",
    "        precision_clase = verdaderos_positivos / total_predichos if total_predichos > 0 else 0\n",
    "        recall_clase = verdaderos_positivos / total_reales if total_reales > 0 else 0\n",
    "        \n",
    "        print(f\"  ‚Ä¢ {clase}:\")\n",
    "        print(f\"    - Predicciones correctas: {verdaderos_positivos}\")\n",
    "        print(f\"    - Precision: {precision_clase:.3f}\")\n",
    "        print(f\"    - Recall: {recall_clase:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pueden generar visualizaciones del an√°lisis supervisado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d1afa",
   "metadata": {},
   "source": [
    "### üîç AN√ÅLISIS NO SUPERVISADO - K-Means Clustering & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851281b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîç AN√ÅLISIS NO SUPERVISADO\n",
    "print(\"üöÄ Iniciando an√°lisis no supervisado...\")\n",
    "\n",
    "if not df_no_supervisado.empty:\n",
    "    \n",
    "    # ==========================================\n",
    "    # PREPARACI√ìN DE DATOS PARA CLUSTERING\n",
    "    # ==========================================\n",
    "    \n",
    "    print(f\"üìä Dataset para clustering: {df_no_supervisado.shape}\")\n",
    "    \n",
    "    # Normalizar datos para clustering\n",
    "    scaler_unsup = StandardScaler()\n",
    "    X_scaled_unsup = scaler_unsup.fit_transform(df_no_supervisado)\n",
    "    \n",
    "    print(\"‚úÖ Datos normalizados para clustering\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # DETERMINACI√ìN DEL N√öMERO √ìPTIMO DE CLUSTERS\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\\\nüîÑ Determinando n√∫mero √≥ptimo de clusters...\")\n",
    "    \n",
    "    # M√©todo del codo\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "    k_range = range(2, 8)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled_unsup)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X_scaled_unsup, kmeans.labels_))\n",
    "    \n",
    "    # ==========================================\n",
    "    # VISUALIZACI√ìN: M√âTODO DEL CODO Y SILHOUETTE\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"üìà Gr√°fico 3: M√©todo del Codo y Puntuaci√≥n de Silhouette\")\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # M√©todo del codo\n",
    "    ax1.plot(k_range, wcss, 'bo-', linewidth=2, markersize=8)\n",
    "    ax1.set_title('M√©todo del Codo', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('N√∫mero de clusters', fontsize=12)\n",
    "    ax1.set_ylabel('WCSS', fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette score\n",
    "    ax2.plot(k_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)\n",
    "    ax2.set_title('Puntuaci√≥n de Silhouette por N√∫mero de Clusters', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('N√∫mero de clusters', fontsize=12)\n",
    "    ax2.set_ylabel('Puntuaci√≥n de Silhouette', fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('metodo_codo_silhouette_etl.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Encontrar k √≥ptimo\n",
    "    optimal_k = k_range[silhouette_scores.index(max(silhouette_scores))]\n",
    "    print(f\"\\\\nüéØ N√∫mero √≥ptimo de clusters: {optimal_k}\")\n",
    "    print(f\"üéØ Mejor Silhouette Score: {max(silhouette_scores):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se puede realizar an√°lisis no supervisado: datos insuficientes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e361268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# APLICAR K-MEANS CON K √ìPTIMO\n",
    "# ==========================================\n",
    "\n",
    "if not df_no_supervisado.empty:\n",
    "    \n",
    "    print(f\"\\\\nüéØ Aplicando K-Means con k={optimal_k}...\")\n",
    "    \n",
    "    # Modelo K-Means final\n",
    "    kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans_final.fit_predict(X_scaled_unsup)\n",
    "    \n",
    "    # Calcular silhouette score final\n",
    "    silhouette_final = silhouette_score(X_scaled_unsup, cluster_labels)\n",
    "    print(f\"‚úÖ Silhouette Score final: {silhouette_final:.3f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # REDUCCI√ìN DE DIMENSIONALIDAD CON PCA\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\\\nüîÑ Aplicando PCA para visualizaci√≥n...\")\n",
    "    \n",
    "    # PCA a 2 componentes para visualizaci√≥n\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled_unsup)\n",
    "    \n",
    "    # Varianza explicada\n",
    "    varianza_explicada = pca.explained_variance_ratio_\n",
    "    print(f\"‚úÖ Varianza explicada por PC1: {varianza_explicada[0]:.3f}\")\n",
    "    print(f\"‚úÖ Varianza explicada por PC2: {varianza_explicada[1]:.3f}\")\n",
    "    print(f\"‚úÖ Varianza total explicada: {sum(varianza_explicada):.3f}\")\n",
    "    \n",
    "    # ==========================================\n",
    "    # VISUALIZACI√ìN: CLUSTERS EN ESPACIO PCA\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"üìà Gr√°fico 4: Visualizaci√≥n de Clusters en Espacio PCA\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Colores para cada cluster\n",
    "    colors = plt.cm.Set1(np.linspace(0, 1, optimal_k))\n",
    "    \n",
    "    for i in range(optimal_k):\n",
    "        mask = cluster_labels == i\n",
    "        plt.scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                   c=[colors[i]], label=f'Cluster {i}', \n",
    "                   alpha=0.7, s=50)\n",
    "    \n",
    "    # Centroides en espacio PCA\n",
    "    centroids_pca = pca.transform(kmeans_final.cluster_centers_)\n",
    "    plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], \n",
    "               c='black', marker='x', s=200, linewidths=3, label='Centroides')\n",
    "    \n",
    "    plt.title('Visualizaci√≥n de Clusters de Luminarias (PCA)', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(f'Componente Principal 1 ({varianza_explicada[0]:.1%} varianza)', fontsize=12)\n",
    "    plt.ylabel(f'Componente Principal 2 ({varianza_explicada[1]:.1%} varianza)', fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('clusters_pca_etl.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # ==========================================\n",
    "    # AN√ÅLISIS DE CLUSTERS\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\\\nüìä AN√ÅLISIS DE CARACTER√çSTICAS POR CLUSTER:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Agregar labels de cluster al dataframe\n",
    "    df_clusters = df_no_supervisado.copy()\n",
    "    df_clusters['cluster'] = cluster_labels\n",
    "    \n",
    "    # Estad√≠sticas por cluster\n",
    "    cluster_stats = df_clusters.groupby('cluster').agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    print(\"üìà Caracter√≠sticas promedio por cluster:\")\n",
    "    display(df_clusters.groupby('cluster').mean().round(2))\n",
    "    \n",
    "    # Distribuci√≥n de clusters\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    print(f\"\\\\nüìä Distribuci√≥n de luminarias por cluster:\")\n",
    "    for i, count in cluster_counts.items():\n",
    "        percentage = (count / len(cluster_labels)) * 100\n",
    "        print(f\"  ‚Ä¢ Cluster {i}: {count:,} luminarias ({percentage:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se pueden generar visualizaciones del an√°lisis no supervisado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9c82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# VISUALIZACI√ìN: DISTRIBUCI√ìN DE CARACTER√çSTICAS POR CLUSTER\n",
    "# ==========================================\n",
    "\n",
    "if not df_no_supervisado.empty and len(df_clusters.columns) > 1:\n",
    "    \n",
    "    print(\"üìà Gr√°fico 5: Distribuci√≥n de Caracter√≠sticas por Cluster\")\n",
    "    \n",
    "    # Seleccionar las variables m√°s importantes para visualizar\n",
    "    variables_importantes = [\n",
    "        'potencia_watts', 'altura_metros', 'consumo.actual_watts',\n",
    "        'sensores.luminosidad_lux', 'sensores.temperatura_c', 'sensores.humedad_pct',\n",
    "        'eficiencia.lumens_por_watt', 'eficiencia.horas_funcionamiento_total',\n",
    "        'eficiencia.vida_util_restante_pct', 'conectividad.latencia_ms'\n",
    "    ]\n",
    "    \n",
    "    # Filtrar variables que existen en el dataset\n",
    "    variables_disponibles = [var for var in variables_importantes if var in df_clusters.columns]\n",
    "    \n",
    "    if len(variables_disponibles) >= 4:\n",
    "        # Crear subplots para boxplots\n",
    "        n_vars = len(variables_disponibles)\n",
    "        n_cols = 4\n",
    "        n_rows = (n_vars + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, 5*n_rows))\n",
    "        if n_rows == 1:\n",
    "            axes = axes.reshape(1, -1)\n",
    "        \n",
    "        for i, var in enumerate(variables_disponibles):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            \n",
    "            ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "            \n",
    "            # Boxplot por cluster\n",
    "            df_clusters.boxplot(column=var, by='cluster', ax=ax)\n",
    "            ax.set_title(var.replace('.', '.\\\\n'))\n",
    "            ax.set_xlabel('Cluster')\n",
    "            ax.set_ylabel(var.split('.')[-1])\n",
    "            \n",
    "        # Ocultar subplots vac√≠os\n",
    "        for i in range(len(variables_disponibles), n_rows * n_cols):\n",
    "            row = i // n_cols\n",
    "            col = i % n_cols\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        plt.suptitle('Distribuci√≥n de Caracter√≠sticas por Cluster', fontsize=16, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('distribucion_clusters_etl.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # ==========================================\n",
    "    # INTERPRETACI√ìN DE CLUSTERS\n",
    "    # ==========================================\n",
    "    \n",
    "    print(\"\\\\nüéØ INTERPRETACI√ìN DE CLUSTERS:\")\n",
    "    print(\"=\"*35)\n",
    "    \n",
    "    cluster_means = df_clusters.groupby('cluster').mean()\n",
    "    \n",
    "    for cluster_id in range(optimal_k):\n",
    "        print(f\"\\\\nüîç **Cluster {cluster_id}:**\")\n",
    "        cluster_data = cluster_means.loc[cluster_id]\n",
    "        \n",
    "        # Identificar caracter√≠sticas destacadas\n",
    "        caracteristicas_altas = []\n",
    "        caracteristicas_bajas = []\n",
    "        \n",
    "        for var in variables_disponibles:\n",
    "            if var in cluster_data.index:\n",
    "                valor = cluster_data[var]\n",
    "                promedio_general = df_clusters[var].mean()\n",
    "                std_general = df_clusters[var].std()\n",
    "                \n",
    "                if valor > promedio_general + 0.5 * std_general:\n",
    "                    caracteristicas_altas.append(f\"{var}: {valor:.2f}\")\n",
    "                elif valor < promedio_general - 0.5 * std_general:\n",
    "                    caracteristicas_bajas.append(f\"{var}: {valor:.2f}\")\n",
    "        \n",
    "        if caracteristicas_altas:\n",
    "            print(f\"   üìà Caracter√≠sticas altas: {', '.join(caracteristicas_altas[:3])}\")\n",
    "        if caracteristicas_bajas:\n",
    "            print(f\"   üìâ Caracter√≠sticas bajas: {', '.join(caracteristicas_bajas[:3])}\")\n",
    "        \n",
    "        # Contar luminarias en este cluster\n",
    "        count = cluster_counts[cluster_id]\n",
    "        percentage = (count / len(cluster_labels)) * 100\n",
    "        print(f\"   üìä Luminarias: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No se puede generar an√°lisis detallado de clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0cef0ac",
   "metadata": {},
   "source": [
    "## üìä RESULTADOS Y CONCLUSIONES DEL ETL\n",
    "\n",
    "### üéØ Resumen del Proceso ETL\n",
    "\n",
    "El proceso ETL implementado ha demostrado ser efectivo para la preparaci√≥n y an√°lisis de datos de luminarias urbanas, integrando exitosamente metodolog√≠as supervisadas y no supervisadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258a808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# RESUMEN FINAL Y RECOMENDACIONES\n",
    "# ==========================================\n",
    "\n",
    "print(\"üìã RESUMEN EJECUTIVO DEL PROCESO ETL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Resumen de datos procesados\n",
    "print(f\"\\\\nüìä DATOS PROCESADOS:\")\n",
    "if not df_raw.empty:\n",
    "    print(f\"  ‚Ä¢ Registros extra√≠dos: {len(df_raw):,}\")\n",
    "    print(f\"  ‚Ä¢ Columnas originales: {df_raw.shape[1]}\")\n",
    "\n",
    "if not df_supervisado.empty:\n",
    "    print(f\"  ‚Ä¢ Registros para an√°lisis supervisado: {len(df_supervisado):,}\")\n",
    "    print(f\"  ‚Ä¢ Caracter√≠sticas supervisadas: {len(df_supervisado.columns)-1}\")\n",
    "\n",
    "if not df_no_supervisado.empty:\n",
    "    print(f\"  ‚Ä¢ Registros para an√°lisis no supervisado: {len(df_no_supervisado):,}\")\n",
    "    print(f\"  ‚Ä¢ Caracter√≠sticas no supervisadas: {len(df_no_supervisado.columns)}\")\n",
    "\n",
    "# Resumen de resultados\n",
    "print(f\"\\\\nüéØ RESULTADOS OBTENIDOS:\")\n",
    "\n",
    "if 'accuracy' in locals():\n",
    "    print(f\"  ‚Ä¢ Accuracy del modelo supervisado: {accuracy:.1%}\")\n",
    "    print(f\"  ‚Ä¢ Precisi√≥n promedio: {precision:.1%}\")\n",
    "\n",
    "if 'silhouette_final' in locals():\n",
    "    print(f\"  ‚Ä¢ Silhouette Score del clustering: {silhouette_final:.3f}\")\n",
    "    print(f\"  ‚Ä¢ N√∫mero √≥ptimo de clusters: {optimal_k}\")\n",
    "\n",
    "# Archivos generados\n",
    "print(f\"\\\\nüìÅ ARCHIVOS GENERADOS:\")\n",
    "archivos_generados = [\n",
    "    \"luminarias_dataset.csv\",\n",
    "    \"importancia_variables_etl.png\", \n",
    "    \"matriz_confusion_etl.png\",\n",
    "    \"metodo_codo_silhouette_etl.png\",\n",
    "    \"clusters_pca_etl.png\",\n",
    "    \"distribucion_clusters_etl.png\"\n",
    "]\n",
    "\n",
    "for archivo in archivos_generados:\n",
    "    if os.path.exists(archivo):\n",
    "        size_mb = os.path.getsize(archivo) / 1024 / 1024\n",
    "        print(f\"  ‚úÖ {archivo} ({size_mb:.2f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è {archivo} (no encontrado)\")\n",
    "\n",
    "# ==========================================\n",
    "# RECOMENDACIONES Y PR√ìXIMOS PASOS\n",
    "# ==========================================\n",
    "\n",
    "print(f\"\\\\nüí° RECOMENDACIONES:\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "print(\"\\\\nüîç **Para An√°lisis Supervisado:**\")\n",
    "print(\"  ‚Ä¢ Considerar recolectar m√°s datos etiquetados\")\n",
    "print(\"  ‚Ä¢ Explorar t√©cnicas de balanceo de clases\")\n",
    "print(\"  ‚Ä¢ Implementar validaci√≥n cruzada\")\n",
    "print(\"  ‚Ä¢ Probar algoritmos alternativos (XGBoost, SVM)\")\n",
    "\n",
    "print(\"\\\\nüéØ **Para An√°lisis No Supervisado:**\")\n",
    "print(\"  ‚Ä¢ Validar clusters con conocimiento del dominio\")\n",
    "print(\"  ‚Ä¢ Considerar algoritmos de clustering alternativos (DBSCAN, Hierarchical)\")\n",
    "print(\"  ‚Ä¢ Explorar m√°s componentes principales\")\n",
    "print(\"  ‚Ä¢ Implementar an√°lisis de outliers\")\n",
    "\n",
    "print(\"\\\\nüîÑ **Para el Proceso ETL:**\")\n",
    "print(\"  ‚Ä¢ Automatizar el pipeline completo\")\n",
    "print(\"  ‚Ä¢ Implementar monitoreo de calidad de datos\")\n",
    "print(\"  ‚Ä¢ Establecer actualizaciones incrementales\")\n",
    "print(\"  ‚Ä¢ Crear alertas para anomal√≠as en los datos\")\n",
    "\n",
    "print(\"\\\\nüéâ **CONCLUSI√ìN:**\")\n",
    "print(\"El proceso ETL ha proporcionado una base s√≥lida para el an√°lisis\")\n",
    "print(\"de luminarias urbanas, identificando patrones importantes y\")\n",
    "print(\"estableciendo m√©tricas de rendimiento para futuros modelos.\")\n",
    "print(\"\\\\n‚úÖ Proceso ETL completado exitosamente!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
